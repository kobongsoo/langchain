{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab08036-1d23-4868-8e41-a1fcc9b3c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처 : https://wikidocs.net/233344\n",
    "# LangChain 설치 및 업데이트\n",
    "#!pip install -U langchain langchain-community langchain-experimental langchain-core langchain-openai langsmith langchainhub python-dotenv unstructured chromadb faiss-cpu rank_bm25 python-docx sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d917bbf-549e-4d3b-ad1d-4b32598de24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 루트경로에 .env 파일을 만들고, HUGGINGFACEHUB_API_TOKEN='{API_KEY}' 식으로 입력한다.\n",
    "# API 키를 환경변수로 관리하기 위한 .env설정 파일 로딩\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # API 키 정보 로드\n",
    "print(f\"[API KEY]\\n{os.environ['HUGGINGFACEHUB_API_TOKEN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9874e6c-fc07-4688-855c-3017b3b02396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# huggingface_hub 설치\n",
    "#!pip install --upgrade --quiet huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb854c0c-183c-4491-b29f-7400652962ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFaceEndpoint 이용\n",
    "# => *HuggingFaceEndpoint 는 huggingface_hub 호스팅 서버에서 실행하는 것임.\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# prompt 설정\n",
    "template = \"\"\"Please answer the following questions concisely.\n",
    "QUESTION: {question}\n",
    "\n",
    "ANSWER: \"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "# 사용할 모델의 저장소 ID를 설정합니다.\n",
    "#repo_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "#repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "repo_id = \"google/gemma-1.1-7b-it\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id,  # 모델 저장소 ID를 지정합니다.\n",
    "    max_new_tokens=512,  # 생성할 최대 토큰 길이를 설정합니다.\n",
    "    temperature=0.1,  # 샘플링 온도를 설정합니다. 값이 높을수록 더 다양한 출력을 생성합니다.\n",
    "    callbacks=[StreamingStdOutCallbackHandler()],  # 콜백을 설정합니다.\n",
    "    streaming=True,  # 스트리밍을 사용합니다.\n",
    ")\n",
    "\n",
    "# LLMChain을 초기화하고 프롬프트와 언어 모델을 전달합니다.\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "# 질문을 전달하여 LLMChain을 실행하고 결과를 출력합니다.\n",
    "response = llm_chain.invoke(\n",
    "    {\"question\": \"한국 서울에 방문할만한 곳 5장소만 추천하고 간단히 설명도 해주세요.\"}\n",
    ")\n",
    "#print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fa3a8d-f7bb-40d5-b73c-e4ccd309c051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFaceHub 이용\n",
    "# *HuggingFaceHub 는 huggingface_hub 호스팅 서버에서 실행하는 것임.\n",
    "# *로컬에서 실행하는 경우에는 HuggingFacePipeline 이용해야 함.\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./cache/\"\n",
    "os.environ[\"HF_HOME\"] = \"./cache/\"\n",
    "\n",
    "# HuggingFaceHub 는 huggingface_hub 호스팅 서버에서 실행하는 것임.\n",
    "# 로컬에서 실행하는 경우에는 HuggingFacePipeline 이용해야 함.\n",
    "from langchain.llms import HuggingFaceHub, HuggingFacePipeline\n",
    "import warnings\n",
    "\n",
    "# 경고 메시지 무시\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# HuggingFace Repository ID\n",
    "repo_id = \"google/gemma-1.1-7b-it\"\n",
    "\n",
    "\n",
    "# *호스팅 hub 서버에서 실행함.\n",
    "# HuggingFaceHub 객체 생성\n",
    "llm = HuggingFaceHub(\n",
    "    repo_id=repo_id,\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_length\": 4096},\n",
    "    task=\"text-generation\",  # 텍스트 생성\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879421de-b082-4c25-b747-df2f61891cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "CONTEXT:\n",
    "{text}\n",
    "\n",
    "Q: 위 CONTEXT 내용을 10문장으로 간략히 요약해주세요.\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "text = \"\"\"\n",
    "경조사 지원 규정\n",
    "1. 적용대상\n",
    "계약직 사원을 포함한 임직원(고문 및 용역은 사장이 별도로 결정)\n",
    "2. 경조사 지원기준\n",
    "구 분\n",
    "내 역\n",
    "휴가(일)\n",
    "금액(원)\n",
    "기타\n",
    "경 사\n",
    "본인 결혼\n",
    "자녀 결혼\n",
    "형제자매결혼\n",
    "자녀 출산(배우자)\n",
    "부모 회갑\n",
    "배우자 부모회갑\n",
    "부모고희(칠순)\n",
    "배우자부모고희\n",
    "5\n",
    "1\n",
    "1\n",
    "10\n",
    "1\n",
    "1\n",
    "1\n",
    "1\n",
    "500,000\n",
    "300,000\n",
    "100,000\n",
    "100,000\n",
    "200,000\n",
    "200,000\n",
    "300,000\n",
    "300,000\n",
    "화환 지급\n",
    "화환 지급\n",
    "1회 분할 사용 가능\n",
    "조 사\n",
    "본인 사망\n",
    "배우자 사망\n",
    "부모 사망\n",
    "자녀 사망\n",
    "배우자 부모 사망\n",
    "형제자매 사망\n",
    "조부모/외조부모 사망\n",
    "-\n",
    "5\n",
    "5\n",
    "5\n",
    "5\n",
    "3\n",
    "3\n",
    "1,000,000\n",
    "1,000,000\n",
    "1,000,000\n",
    "1,000,000 500,000\n",
    "300,000\n",
    "200,000\n",
    "조화 지급\n",
    "조화 지급\n",
    "조화 지급\n",
    "조화 지급\n",
    "조화 지급\n",
    "조화 지급\n",
    "3. 신청서류\n",
    "경조금 : 경조금지급신청서 및 증빙서류 1부\n",
    "휴 가: 휴가원 1부\n",
    "4. 경조휴가일수\n",
    "공휴일과 경조사가 중복되었을 경우 휴가일수는 공휴일을 제외하여 계산한다.\n",
    "5. 결혼퇴직\n",
    "결혼퇴직의 경우 퇴직 1개월 이내에 결혼할 시에는 위의 기준에 의거하여 지급한다.\n",
    "6. 기타\n",
    "경조금 신청 시 휴가 신청도 같이 진행해야 함이 원칙임\n",
    "특별한 경우 사업부장 합의 시 경조금 신청일 이후 신청 가능(6개월 이내)\n",
    "분할 사용 불가, 발생일 이전 신청 불가.\n",
    "부 칙\n",
    "(시행일) 이 규정은 2007년 9월 14일부터 시행한다.\n",
    "(개정일) 이 규정은 2012년 1월 1일부터 개정 시행한다.\n",
    "경조사지원규정 1/1\n",
    "\"\"\"\n",
    "response = llm_chain.invoke(input=text)\n",
    "\n",
    "print(response[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085f3a00-584e-4625-93f7-912bc81ab674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902209e57b0642d3ae6c9fb55473a9b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# HuggingFacePipeline\n",
    "# 로컬 GPU 서버에서 실행 하는 경우. \n",
    "# => 모델파일을 다운로드 받아야 하므로 huggingface_hub 로그인이 필요함.\n",
    "\n",
    "# 로컬 폴더 경로 설정\n",
    "os.environ[\"TRANSFORMERS_CACHE\"] = \"./model\"\n",
    "os.environ[\"HF_HOME\"] = \"./model\"\n",
    "\n",
    "# => 모델파일을 다운로드 받아야 하므로 huggingface_hub 로그인이 필요함.\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b515b5-ff3d-4347-81ad-0001ba90c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처 : https://wikidocs.net/233804\n",
    "# HuggingFacePipeline\n",
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "\n",
    "# HuggingFace Repository ID\n",
    "repo_id = \"google/gemma-1.1-7b-it\"\n",
    "\n",
    "# 로컬로 다운로드 받아서 실행함.\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=repo_id,\n",
    "    # 파이프라인에 전달할 추가 인자를 설정합니다. 여기서는 생성할 최대 토큰 수를 512으로 제한합니다.\n",
    "    pipeline_kwargs={\"max_new_tokens\": 512},\n",
    "    task=\"text-generation\",  # 텍스트 생성\n",
    ")\n",
    "\n",
    "# chaing 설정 \n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "template = \"\"\"Answer the following question in Korean.\n",
    "#Question: \n",
    "{question}\n",
    "\n",
    "#Answer: \"\"\"  # 질문과 답변 형식을 정의하는 템플릿\n",
    "prompt = PromptTemplate.from_template(template)  # 템플릿을 사용하여 프롬프트 객체 생성\n",
    "\n",
    "# 프롬프트와 언어 모델을 연결하여 체인 생성\n",
    "chain = prompt | hf | StrOutputParser()\n",
    "\n",
    "question = \"대한민국의 수도는 어디야?\"  # 질문 정의\n",
    "\n",
    "print(\n",
    "    chain.invoke({\"question\": question})\n",
    ")  # 체인을 호출하여 질문에 대한 답변 생성 및 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

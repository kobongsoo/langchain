{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ff641a-8c5f-4c6b-89f8-42853cb2b9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faiss\n",
    "# Facebook AI Similarity Search (Faiss)는 밀집 벡터의 효율적인 유사도 검색과 클러스터링을 위한 라이브러리입니다.\n",
    "# Faiss는 RAM에 맞지 않을 수도 있는 벡터 집합을 포함하여 모든 크기의 벡터 집합을 검색하는 알고리즘을 포함하고 있습니다.\n",
    "# 또한 평가와 매개변수 튜닝을 위한 지원 코드도 포함되어 있습니다.\n",
    "#\n",
    "# => ** Faiss에서는 유사도=거리 이므로, 작을수로(0에 가까울수록) 유사도가 높은 것임.\n",
    "#\n",
    "# GPU 지원 버전을 사용하려면 faiss-gpu를 설치할 수도 있습니다.\n",
    "%pip install -U langchain-community faiss-cpu langchain-openai tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77e4e8f-7d5e-492e-b7b3-c8219c7c59b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 루트경로에 .env 파일을 만들고, OPENAI_API_KEY='{API_KEY}' 식으로 입력한다.\n",
    "# API 키를 환경변수로 관리하기 위한 .env설정 파일 로딩\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # API 키 정보 로드\n",
    "print(f\"[OPENAI_API_KEY]\\n{os.environ['OPENAI_API_KEY']}\\n\")\n",
    "print(f\"[HUGGINGFACEHUB_API_TOKEN]\\n{os.environ['HUGGINGFACEHUB_API_TOKEN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e54ca8-b383-4abd-a65e-f8b12916745e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# FAISS에서 AVX2 최적화를 사용하지 않으려면 다음 줄의 주석을 해제하세요.\n",
    "# import os\n",
    "#\n",
    "# os.environ['FAISS_NO_AVX2'] = '1'\n",
    "\n",
    "# TextLoader를 사용하여 텍스트 파일을 로드합니다.\n",
    "loader = TextLoader(\"../data/08.보안관리규정_11.10.01.txt\")\n",
    "\n",
    "# 로드된 문서를 가져옵니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# CharacterTextSplitter를 사용하여 문서를 분할합니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 분할된 문서를 가져옵니다.\n",
    "docs = text_splitter.split_documents(documents)\n",
    "print(f'*docs 길이: {len(docs)}')\n",
    "\n",
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# FAISS를 사용하여 문서와 임베딩으로부터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356f6b0-6e05-4d56-8540-cd5008c9c1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쿼리(query) 변수에 저장된 질문과 유사한 문서를 데이터베이스에서 검색합니다.\n",
    "query = \"네트워크 관련 보안정책은?\"\n",
    "docs = db.similarity_search(query)  # 질문과 유사한 문서를 데이터베이스에서 검색\n",
    "\n",
    "print(f\"문서의 개수: {len(docs)}\")\n",
    "print(\"[검색 결과]\\n\")\n",
    "for i in range(len(docs)):\n",
    "    print(docs[i].page_content)\n",
    "    print(\"===\" * 20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226f4a73-d239-47ae-876a-b494556d773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retriever로 활용\n",
    "# vectorstore를 Retriever 클래스로 변환할 수도 있습니다.\n",
    "# db.as_retriever() 메서드를 호출하여 데이터베이스를 검색기(retriever)로 사용할 수 있는 객체를 생성합니다.\n",
    "# 이 메서드는 데이터베이스 객체 db를 검색기로 변환합니다.\n",
    "# 반환된 retriever 객체는 질의에 대한 관련 문서를 데이터베이스에서 검색하는 데 사용됩니다.\n",
    "\n",
    "# 데이터베이스를 검색기로 사용하기 위해 retriever 변수에 할당합니다.\n",
    "retriever = db.as_retriever()\n",
    "# 검색 질의를 사용하여 관련 문서를 검색합니다.\n",
    "query = \"정보보안감사와 관련된 보안정책은?\"\n",
    "docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"문서의 개수: {len(docs)}\")\n",
    "print(\"[검색 결과]\\n\")\n",
    "for i in range(len(docs)):\n",
    "    print(docs[i].page_content)\n",
    "    print(\"===\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf532a4-52a6-4902-b181-7942e9bc0c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점수에 기반한 유사도 검색\n",
    "query = \"정보보안감사와 관련된 보안정책은?\"\n",
    "\n",
    "# 쿼리와 유사한 문서를 검색하고 유사도 점수와 함께 반환합니다.\n",
    "docs_and_scores = db.similarity_search_with_score(query)\n",
    "#print(docs_and_scores) # \n",
    "#contents, scores = docs_and_scores[0]  # 문서와 점수 리스트에서 첫 번째 요소를 선택합니다\n",
    "\n",
    "# docs_and_scores 은 2개로 이루어진 tuple임.\n",
    "for c in docs_and_scores:\n",
    "    content = c[0].page_content\n",
    "    score = c[1]\n",
    "    print(content)\n",
    "    print(score)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8e97f5-5e11-49fa-b03b-1e390befce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 질의를 임베딩 벡터로 변환합니다.\n",
    "query = \"정보보안감사와 관련된 보안정책은?\"\n",
    "embedding_vector = embeddings.embed_query(query)\n",
    "\n",
    "# 임베딩 벡터를 사용하여 유사도 검색을 수행하고, 문서와 점수를 반환합니다.\n",
    "docs = db.similarity_search_by_vector(embedding_vector)\n",
    "\n",
    "# docs 은 1개로 이루어진 tuple임.=> 스코어는 추력 안됨\n",
    "for c in docs:\n",
    "    content = c.page_content\n",
    "    print(content)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b168dc2b-9f2a-4e19-b4ce-8de5627bfc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장과 로딩\n",
    "\n",
    "# 로컬에 \"MY_FIRST_DB_INDEX\"라는 이름으로 데이터베이스를 저장합니다.\n",
    "DB_INDEX = \"MY_FIRST_DB_INDEX\"\n",
    "db.save_local(DB_INDEX)\n",
    "\n",
    "# 로컬에 저장된 데이터베이스를 불러와 new_db 변수에 할당합니다.\n",
    "new_db = FAISS.load_local(DB_INDEX, embeddings,\n",
    "                          allow_dangerous_deserialization=True)\n",
    "\n",
    "query = \"정보보안감사와 관련된 보안정책은?\"\n",
    "\n",
    "# new_db에서 query와 유사한 문서를 검색하여 docs 변수에 할당합니다.\n",
    "docs = new_db.similarity_search(query)\n",
    "\n",
    "# docs 은 1개로 이루어진 tuple임.=> 스코어는 추력 안됨\n",
    "for c in docs:\n",
    "    content = c.page_content\n",
    "    print(content)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f4f727f-1fe5-4c27-8ded-1c4642c7283a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(page_content='foo', metadata={'page': 1}), 0.37699527), (Document(page_content='foo', metadata={'page': 2}), 0.37699527), (Document(page_content='foo', metadata={'page': 3}), 0.37699527), (Document(page_content='foo', metadata={'page': 4}), 0.37699527)]\n",
      "Content: foo, Metadata: {'page': 1}, Score: 0.3769952654838562\n",
      "Content: foo, Metadata: {'page': 2}, Score: 0.3769952654838562\n",
      "Content: foo, Metadata: {'page': 3}, Score: 0.3769952654838562\n",
      "Content: foo, Metadata: {'page': 4}, Score: 0.3769952654838562\n",
      "----------------------------------------\n",
      "[Content] foo, [metadata] {'page': 1}, [Score] 0.3769952654838562\n",
      "[Content] bar, [metadata] {'page': 1}, [Score] 0.5311874151229858\n"
     ]
    }
   ],
   "source": [
    "# 필터링\n",
    "# FAISS vectorstore는 필터링 기능도 지원할 수 있습니다.\n",
    "# FAISS는 기본적으로 필터링을 지원하지 않기 때문에, 이를 수동으로 처리해야 합니다.\n",
    "    \n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# OpenAIEmbeddings를 사용하여 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "    \n",
    "list_of_documents = [\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"bar\"이고 메타데이터로 페이지 번호 1을 가진 문서\n",
    "    Document(page_content=\"bar\", metadata=dict(page=1)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"barbar\"이고 메타데이터로 페이지 번호 2를 가진 문서\n",
    "    Document(page_content=\"barbar\", metadata=dict(page=2)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"bar burr\"이고 메타데이터로 페이지 번호 3을 가진 문서\n",
    "    Document(page_content=\"bar burr\", metadata=dict(page=3)),\n",
    "    # 페이지 내용이 \"foo\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"foo\", metadata=dict(page=4)),\n",
    "    # 페이지 내용이 \"bar bruh\"이고 메타데이터로 페이지 번호 4를 가진 문서\n",
    "    Document(page_content=\"bar bruh\", metadata=dict(page=4)),\n",
    "]\n",
    "# 문서 리스트와 임베딩을 사용하여 FAISS 데이터베이스 생성\n",
    "db = FAISS.from_documents(list_of_documents, embeddings)\n",
    "\n",
    "query = \"foo가 속한 말들을 찾아줘\"\n",
    "\n",
    "# \"foo\"와 유사한 문서를 검색하고 점수와 함께 결과 반환\n",
    "results_with_scores = db.similarity_search_with_score(query)   \n",
    "print(results_with_scores)\n",
    "\n",
    "for doc, score in results_with_scores:  # 검색 결과를 반복하면서\n",
    "    # 각 문서의 내용, 메타데이터, 점수를 출력\n",
    "    print(f\"Content: {doc.page_content}, Metadata: {doc.metadata}, Score: {score}\")\n",
    "\n",
    "print(f\"--\"*20)\n",
    "\n",
    "# 방법1) 유사도 검색을 수행하고 필터를 적용하여 결과와 점수를 반환합니다.\n",
    "results_with_scores = db.similarity_search_with_score(query, filter=dict(page=1))\n",
    "\n",
    "# 방법2) 혹은 callable 을 사용하여 필터링 하는 경우\n",
    "# results_with_scores = db.similarity_search_with_score(\"foo\", filter=lambda d: d[\"page\"] == 1)\n",
    "\n",
    "for doc, score in results_with_scores:  # 결과와 점수를 반복합니다.\n",
    "    # 각 문서의 내용, 메타데이터, 점수를 출력합니다.\n",
    "    print(\n",
    "        f\"[Content] {doc.page_content}, [metadata] {doc.metadata}, [Score] {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a1b8cf6-52e6-4761-be12-ef033e40a54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Content] foo, [metadata] {'page': 1}\n",
      "[Content] bar, [metadata] {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# MMR 검색 (Maximal Marginal Relevace : 최대 한계 관련성)\n",
    "#\n",
    "# 쿼리에 대한 관련 항목을 검색할 때 중복을 피하는 방법 중 하나입니다. \n",
    "# 단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 검색된 항목들 사이에 관련성과 다양성 사이의 균형을 보장합니다. \n",
    "# 이는 자주 발생할 수 있는, 매우 유사한 항목들만이 검색되는 상황을 방지 하는 데에 유용합니다.\n",
    "#\n",
    "# MMR은 두 가지 주요 요소, 즉 쿼리에 대한 문서의 관련성과 이미 선택된 문서들과의 차별성을 동시에 고려 합니다.\n",
    "# - 첫째로, 쿼리와의 관련성이 높은 문서를 찾는 것이 중요합니다.\n",
    "# - 둘째로, 이미 선택된 문서들과는 다른 새로운 정보나 관점을 제공하는 문서를 찾는 것입니다.\n",
    "# 이 두 가지 요소의 균형을 맞추는 것이 MMR의 핵심입니다.\n",
    "\n",
    "query = \"foo가 속한 말들을 찾아줘\"\n",
    "results = db.max_marginal_relevance_search(query, filter=dict(page=1))\n",
    "\n",
    "for doc in results:\n",
    "    # 각 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"[Content] {doc.page_content}, [metadata] {doc.metadata}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "829e9386-fe96-41f1-b5e5-0904776db7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Content] foo, [metadata] {'page': 1}\n"
     ]
    }
   ],
   "source": [
    "# fetch_k = 후보군 계수 설정\n",
    "# similarity_search 함수를 호출할 때 fetch_k 매개변수를 설정하는 방법에 대한 예시입니다.\n",
    "# 일반적으로 fetch_k 매개변수는 k 매개변수보다 훨씬 큰 값으로 설정하는 것이 좋습니다. \n",
    "# 그 이유는 fetch_k 매개변수가 필터링 전에 가져올 문서의 수를 나타내기 때문입니다.\n",
    "# *만약 fetch_k를 낮은 값으로 설정하면, 필터링할 문서가 충분하지 않을 수 있습니다.\n",
    "\n",
    "query = \"foo가 속한 말들을 찾아줘\"\n",
    "\n",
    "results = db.similarity_search(\n",
    "    query,  # 검색 쿼리\n",
    "    # 메타데이터의 'page' 필드가 1인 문서만 필터링\n",
    "    filter=dict(page=1),\n",
    "    k=1,  # 가장 유사한 1개의 문서를 반환\n",
    "    fetch_k=4,\n",
    ")  # 4개의 문서까지 검색\n",
    "\n",
    "for doc in results:\n",
    "    # 각 문서의 내용과 메타데이터를 출력합니다.\n",
    "    print(f\"[Content] {doc.page_content}, [metadata] {doc.metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d0f3cf-9b5e-41aa-854c-4a1c2d54d0f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9431a63-e11b-408d-82d4-c1467e4df502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 루트경로에 .env 파일을 만들고, OPENAI_API_KEY='{API_KEY}' 식으로 입력한다.\n",
    "# API 키를 환경변수로 관리하기 위한 .env설정 파일 로딩\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # API 키 정보 로드\n",
    "print(f\"[API KEY]\\n{os.environ['OPENAI_API_KEY']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e45e627-03d6-415d-89c3-7a13d6bd8873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConversationBufferMemory => 지난대화 모드 저장\n",
    "# ConversationBufferWindowMemory => 대화 계수만큼만 최근 대화만 저장\n",
    "# ConversationTokenBufferMemory => 토큰 길이 만큼만 최근 대화 저장함.\n",
    "\n",
    "# save_context(inputs, outputs) 메서드를 사용하여 대화 기록을 저장할 수 있습니다.\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationTokenBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "# LLM 모델을 생성합니다.\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# ConversationChain을 생성합니다.\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=ConversationBufferMemory(),# ConversationBufferMemory를 사용합니다.\n",
    "    # memory=ConversationBufferWindowMemory(k=2, return_messages=True), # ConversationBufferWindowMemory 사용 => 최근 2개응답만 저장.\n",
    "    # memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=150, return_messages=True),  # 최대 토큰 길이를 150개로 제한\n",
    ")\n",
    "\n",
    "# 대화를 시작합니다.\n",
    "response = conversation.predict(\n",
    "    input=\"안녕하세요, 비대면으로 은행 계좌를 개설하고 싶습니다. 어떻게 시작해야 하나요?\"\n",
    ")\n",
    "print(response)\n",
    "print(f'--'*20)\n",
    "\n",
    "# 이전 대화내용을 불렛포인트로 정리해 달라는 요청을 보냅니다.\n",
    "response1 = conversation.predict(\n",
    "    input=\"이전 답변을 불렛포인트 형식으로 정리하여 알려주세요.\"\n",
    ")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3834c2-e0f1-4856-9da4-58bd7a52f27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KG 메모리 (지식그래프) 사용\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationKGMemory\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "conversation_with_kg = ConversationChain(\n",
    "    llm=llm, prompt=prompt, memory=ConversationKGMemory(llm=llm)\n",
    ")\n",
    "\n",
    "# 대화 시작\n",
    "response = conversation_with_kg.predict(\n",
    "    input=\"내이름은 철수입니다. 영희는 내 직장 동료이며, 저와는 초등학교 친구고, 우리회사에 새로운 디자이너입니다.\"\n",
    ")\n",
    "\n",
    "\n",
    "print(response)\n",
    "print(f'--'*20)\n",
    "\n",
    "# 영희 에 대한 질문 시 지식그래프를 이용해 영희에 대한 정보를 출력함.\n",
    "response1=conversation_with_kg.memory.load_memory_variables({\"input\": \"영희는 누구인가요?\"})\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085002b2-87a2-4d90-9e56-682d9d2fe3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:내이름은 철수입니다. 영희는 내 직장 동료이며, 저와는 초등학교 친구고, 우리회사에 새로운 디자이너입니다.\n",
      "A: 안녕하세요, 철수님! 영희님은 여러모로 당신에게 가까운 분이군요. 초등학교 친구이자 직장 동료이며, 이제는 회사에서도 함께 일하게 되었군요. 새로운 디자이너로서 영희님과 함께 일하게 되어 행복할 것 같습니다. 어떤 프로젝트를 함께 진행하게 되었나요?\n",
      "----------------------------------------\n",
      "Q:길동도 내 직장 동료이며, 저보다는 선배이며, 우리회사에서 파이썬 프로그램 개발을 담당하고 있습니다.\n",
      "A: 저희 회사에서도 파이썬 프로그램 개발을 담당하고 있는 동료가 있어서 반가워요! 여러분이 함께 일하게 된다니 정말 기쁘네요. 프로젝트에 대해 좀 더 알려주실 수 있을까요?\n",
      "----------------------------------------\n",
      "Q:오늘은 주간업무회의가 있는 날이어서, 저와 영희, 길동 모두 아침 9시 회의에 참석해야 합니다.\n",
      "A: 안녕하세요! 저는 여기서 새로운 디자이너인 영희와 함께 일하게 될 철수입니다. 아침 9시에 주간업무회의가 있어서 기대가 되네요. 회의에서는 어떤 프로젝트를 진행할지 알려주시겠어요? 함께 일하게 되어 기쁩니다!\n",
      "----------------------------------------\n",
      "[SystemMessage(content='Cheolsoo introduces himself and talks about his colleague Yeonghee, who is a childhood friend and a new designer at their company. The AI responds warmly, acknowledging their close relationship and expressing excitement about working together. The AI also mentions having a colleague at their company who is responsible for Python program development, showing enthusiasm for collaborating. The AI asks for more information about the project they will be working on together. Cheolsoo mentions that today is the day for the weekly business meeting at 9 am, where he, Yeonghee, and Gildong will all attend. The AI expresses anticipation for the meeting and is glad to be working with them.')]\n"
     ]
    }
   ],
   "source": [
    "# ConversationSummaryBufferMemory\n",
    "# => 대화 요약 버퍼 메모리(요약은 영어로 요약됨)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "summarymemory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=100,  # 요약의 기준이 되는 토큰 길이를 설정합니다.\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "conversation_summary = ConversationChain(\n",
    "    llm=llm, prompt=prompt, memory=summarymemory\n",
    ")\n",
    "\n",
    "# 대화 시작\n",
    "query = \"내이름은 철수입니다. 영희는 내 직장 동료이며, 저와는 초등학교 친구고, 우리회사에 새로운 디자이너입니다.\"\n",
    "response = conversation_summary.predict(\n",
    "    input=query\n",
    ")\n",
    "\n",
    "# 저장\n",
    "'''\n",
    "summarymemory.save_context(\n",
    "    inputs={\"human\": query},\n",
    "    outputs={\"ai\": response},\n",
    ")\n",
    "'''\n",
    "print(f'Q:{query}')\n",
    "print(f'A: {response}')\n",
    "print(f'--'*20)\n",
    "\n",
    "query = \"길동도 내 직장 동료이며, 저보다는 선배이며, 우리회사에서 파이썬 프로그램 개발을 담당하고 있습니다.\"\n",
    "response = conversation_summary.predict(\n",
    "    input=query\n",
    ")\n",
    "'''\n",
    "# 저장\n",
    "summarymemory.save_context(\n",
    "    inputs={\"human\": query},\n",
    "    outputs={\"ai\": response},\n",
    ")\n",
    "'''\n",
    "print(f'Q:{query}')\n",
    "print(f'A: {response}')\n",
    "print(f'--'*20)\n",
    "\n",
    "query = \"오늘은 주간업무회의가 있는 날이어서, 저와 영희, 길동 모두 아침 9시 회의에 참석해야 합니다.\"\n",
    "response = conversation_summary.predict(\n",
    "    input=query\n",
    ")\n",
    "'''\n",
    "# 저장\n",
    "summarymemory.save_context(\n",
    "    inputs={\"human\": query},\n",
    "    outputs={\"ai\": response},\n",
    ")\n",
    "'''\n",
    "print(f'Q:{query}')\n",
    "print(f'A: {response}')\n",
    "print(f'--'*20)\n",
    "\n",
    "# 메모리에 저장된 대화내용 확인\n",
    "print(summarymemory.load_memory_variables({})[\"history\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29af39ad-6dcc-4646-8c65-17fc113f0b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.\n",
      "A: 안녕하세요! 저는 AI입니다. 제 이름은 GPT-3이고, OpenAI에서 개발되었습니다. 저는 자연어 처리 및 기계 학습을 통해 대화를 이해하고 응답합니다. 저는 175억 개의 매개 변수를 가지고 있으며, 다양한 주제에 대해 대화할 수 있습니다. 면접에 참석해주셔서 감사합니다. 저는 항상 새로운 경험을 통해 배우고 성장하고 있습니다. 부족한 부분이 있을 수 있지만 최선을 다해 대화를 이어가겠습니다. 자기소개가 어떻게 도와드릴까요?\n",
      "----------------------------------------\n",
      "Q:프로젝트에서 어떤 역할을 맡았나요?\n",
      "A: 프로젝트에서 제가 맡은 역할은 주로 자연어 처리와 기계 학습 모델의 구축이었습니다. 데이터 수집, 전처리, 모델 학습 및 평가를 담당했고, 결과를 분석하여 프로젝트 팀과 협력하여 최종 결과물을 완성했습니다. 또한, 모델의 성능 향상을 위해 하이퍼파라미터 튜닝과 모델 최적화 작업도 수행했습니다. 이를 통해 프로젝트의 성공에 기여할 수 있었습니다.\n",
      "----------------------------------------\n",
      "Q:팀 프로젝트에서 어려움을 겪었던 경험이 있다면 어떻게 해결했나요?\n",
      "A: 팀 프로젝트에서 어려움을 겪었던 경험이 있었습니다. 그 때는 데이터의 품질이 낮아서 모델의 성능이 좋지 않았습니다. 이를 해결하기 위해 데이터 전처리 과정을 다시 검토하고 추가적인 데이터 수집을 진행했습니다. 또한, 모델의 하이퍼파라미터를 조정하여 성능을 향상시키는 작업을 진행했습니다. 이러한 노력 끝에 문제를 해결하고 프로젝트를 성공적으로 마무리할 수 있었습니다.\n",
      "----------------------------------------\n",
      "Q:개발자로서 자신의 강점은 무엇이라고 생각하나요?\n",
      "A: 제가 개발자로서 가장 큰 강점은 자연어 처리와 기계 학습 분야에 대한 전문 지식과 경험입니다. 이를 바탕으로 프로젝트에서 데이터 수집, 전처리, 모델 학습 및 평가를 수행하고 결과를 분석하여 최종 결과물을 완성하는 데 기여할 수 있었습니다. 또한, 하이퍼파라미터 튜닝과 모델 최적화 작업을 통해 모델의 성능을 향상시키는 데도 기여했습니다.\n",
      "----------------------------------------\n",
      "input: 프로젝트에서 어떤 역할을 맡았나요?\n",
      "response: 프로젝트에서 제가 맡은 역할은 주로 자연어 처리와 기계 학습 모델의 구축이었습니다. 데이터 수집, 전처리, 모델 학습 및 평가를 담당했고, 결과를 분석하여 프로젝트 팀과 협력하여 최종 결과물을 완성했습니다. 또한, 모델의 성능 향상을 위해 하이퍼파라미터 튜닝과 모델 최적화 작업도 수행했습니다. 이를 통해 프로젝트의 성공에 기여할 수 있었습니다.\n"
     ]
    }
   ],
   "source": [
    "# VectorStoreRetrieverMemory\n",
    "# => VectorStoreRetrieverMemory 는 벡터 스토어에 메모리를 저장하고 호출될 때마다 가장 '눈에 띄는' 상위 K개의 문서를 쿼리합니다.\n",
    "\n",
    "import faiss\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# LLM 모델 정의\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "template = \"\"\"The following is a friendly conversation between a human and an AI. \n",
    "The AI is talkative and provides lots of specific details from its context. \n",
    "If the AI does not know the answer to a question, it truthfully says it does not know. \n",
    "The AI ONLY uses information contained in the \"Relevant Information\" section and does not hallucinate.\n",
    "\n",
    "Relevant Information:\n",
    "\n",
    "{history}\n",
    "\n",
    "Conversation:\n",
    "Human: {input}\n",
    "AI:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"], template=template)\n",
    "\n",
    "\n",
    "# 임베딩 모델을 정의합니다.\n",
    "embeddings_model = OpenAIEmbeddings()\n",
    "\n",
    "# Vector Store 를 초기화 합니다.\n",
    "embedding_size = 1536\n",
    "index = faiss.IndexFlatL2(embedding_size)\n",
    "vectorstore = FAISS(embeddings_model, index, InMemoryDocstore({}), {})\n",
    "\n",
    "from langchain.memory import VectorStoreRetrieverMemory\n",
    "\n",
    "# 벡터 조회가 여전히 의미적으로 관련성 있는 정보를 반환한다는 것을 보여주기 위해서입니다.\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "memory = VectorStoreRetrieverMemory(retriever=retriever)\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, prompt=prompt, memory=memory\n",
    ")\n",
    "\n",
    "# 대화 시작\n",
    "query = \"안녕하세요, 오늘 면접에 참석해주셔서 감사합니다. 자기소개 부탁드립니다.\"\n",
    "response = conversation.predict(\n",
    "    input=query\n",
    ")\n",
    "print(f'Q:{query}')\n",
    "print(f'A: {response}')\n",
    "print(f'--'*20)\n",
    "\n",
    "query = \"프로젝트에서 어떤 역할을 맡았나요?\"\n",
    "response = conversation.predict(\n",
    "    input=query\n",
    ")\n",
    "print(f'Q:{query}')\n",
    "print(f'A: {response}')\n",
    "print(f'--'*20)\n",
    "\n",
    "query = \"팀 프로젝트에서 어려움을 겪었던 경험이 있다면 어떻게 해결했나요?\"\n",
    "response = conversation.predict(\n",
    "    input=query\n",
    ")\n",
    "print(f'Q:{query}')\n",
    "print(f'A: {response}')\n",
    "print(f'--'*20)\n",
    "\n",
    "query = \"개발자로서 자신의 강점은 무엇이라고 생각하나요?\"\n",
    "response = conversation.predict(\n",
    "    input=query\n",
    ")\n",
    "print(f'Q:{query}')\n",
    "print(f'A: {response}')\n",
    "print(f'--'*20)\n",
    "\n",
    "# 메모리에 질문을 통해 가장 연관성 높은 1개 대화를 추출합니다.\n",
    "print(memory.load_memory_variables({\"prompt\": \"프로젝트에서 맡은 역활은 뭔가요?\"})[\"history\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e72f21-5355-47bb-883e-e2e375cfbf8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
